{
    "prank": 1,
    "actv": "shifted_gelu",
    "alpha": 2e-1,
    "d_model": 1600,
    "num_layers": 48,
    "max_len": 512,
    "shifted_bias_factor": 0.65,
    "fine_tuning": true,
    "max_pooling": true,
    "seq_sampling": true,
    "loc_coefficient": 0.001,
    "recon_coefficient": 0.001,
    "edit_tokens": "first",
    "edit_layer": 46,
    "label_smoothing": 0,
    "edit_lr": 5e-2,
    "edit_epoch": 5,
    "weight_names": ["LoRA_A", "LoRA_B"],
    "weight_init": true,
    "ft_weight_names": ["new_LoRA_A", "new_LoRA_B"],
    "bias_init": true,
    "ft_bias_name": "new_kn_bias",
    "device": "cuda",
    "forward_method": "iterative",
    "saved_path": "./checkpoints/kv/new_lora_model.bin",
    "result_path": "./results/kv/",
    "aux_path": "./stats/kv/",
    "embedding_weight": "lm_head.weight.data",
    "value_weight": "c_proj.weight.data",
    "key_weight": "c_fc.weight.data",
    "mlp_layers": "transformer.h.{}.mlp",
    "name": "gpt2-xl"
}